{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"what's (3 + 5) x 12?\", additional_kwargs={}, response_metadata={}, id='a0cf4475-eb91-4076-b570-84319b22d6e5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_AQVSWvE4DsDYIAHYqydjyGTs', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 147, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b30da85f-9545-4230-9cdc-3743738f74a0-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_AQVSWvE4DsDYIAHYqydjyGTs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 147, 'output_tokens': 18, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='8', name='add', id='fa344fc6-36f4-49e3-b8d6-1e5d600f1d93', tool_call_id='call_AQVSWvE4DsDYIAHYqydjyGTs'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_21eAlrxYb2xOaBEhxVbCM9NF', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 172, 'total_tokens': 190, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e0607acd-ce8b-4e83-b24f-378e323a9e48-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_21eAlrxYb2xOaBEhxVbCM9NF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 172, 'output_tokens': 18, 'total_tokens': 190, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='96', name='multiply', id='856871e6-1d8c-403b-824d-cacb1033b152', tool_call_id='call_21eAlrxYb2xOaBEhxVbCM9NF'), AIMessage(content='The result of (3 + 5) x 12 is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 197, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6e16421-cbaf-4260-a3e3-b570f19b4424-0', usage_metadata={'input_tokens': 197, 'output_tokens': 18, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "{'messages': [HumanMessage(content='what is the weather in nyc?', additional_kwargs={}, response_metadata={}, id='964c44bc-c04f-4401-9215-e8ca02f53493'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5x33WQBmBxMl1dpigxKV4Snm', 'function': {'arguments': '{\"location\":\"New York City\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 143, 'total_tokens': 160, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f4a609ab-5519-409b-8796-56efb9834b12-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'New York City'}, 'id': 'call_5x33WQBmBxMl1dpigxKV4Snm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 143, 'output_tokens': 17, 'total_tokens': 160, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content=\"It's always sunny in New York\", name='get_weather', id='e8562690-0400-4f53-b221-1f3b13dc3c55', tool_call_id='call_5x33WQBmBxMl1dpigxKV4Snm'), AIMessage(content='The weather in New York City is sunny.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 173, 'total_tokens': 184, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-0fedc3b4-da3a-4d69-927f-2795ea621ff4-0', usage_metadata={'input_tokens': 173, 'output_tokens': 11, 'total_tokens': 184, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "{'messages': [HumanMessage(content='langgraph에서 adaptive rag를 구축하는 방법을 알려주세요.', additional_kwargs={}, response_metadata={}, id='598d0cff-0a31-4879-9fc5-27fbb26ace26'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_oqqHIAbJJBn1j7KcXBrMZvmw', 'function': {'arguments': '{\"query\":\"langgraph adaptive rag 구축 방법\"}', 'name': 'retrieve'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 148, 'total_tokens': 167, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-97be568b-172f-49b8-b3a4-a44cd24128e4-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'langgraph adaptive rag 구축 방법'}, 'id': 'call_oqqHIAbJJBn1j7KcXBrMZvmw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 148, 'output_tokens': 19, 'total_tokens': 167, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='[\"[page 0] Adaptive RAG\\\\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\\\\nIn the paper, they report query analysis to route across:\\\\nNo Retrieval\\\\nSingle-shot RAG\\\\nIterative RAG\\\\nLet\\'s build on this using LangGraph.\\\\nIn our implementation, we will route between:\\\\nWeb search: for questions related to recent events\\\\nSelf-corrective RAG: for questions related to our index\\\\nSetup\\\\nFirst, let\\'s install our required packages and set our API keys\\\\nIn [1]: %%capture --no-stderr\\\\n%pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb l\\\\nIn [ ]: import getpass\\\\nimport os\\\\ndef _set_env(var: str):\\\\nif not os.environ.get(var):\\\\nos.environ[var] = getpass.getpass(f\\\\\"{var}: \\\\\")\\\\n_set_env(\\\\\"OPENAI_API_KEY\\\\\")\\\\n# _set_env(\\\\\"COHERE_API_KEY\\\\\")\\\\n_set_env(\\\\\"TAVILY_API_KEY\\\\\")\\\\nSet up LangSmith for LangGraph development\\\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets\\\\nyou use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get\\\\nstarted here.\\\\nCreate Index\\\\nIn [ ]: ### Build Index\\\\n\", \"[page 8] ---ROUTE QUESTION---\\\\n---ROUTE QUESTION TO RAG---\\\\n---RETRIEVE---\\\\n\\\\\"Node \\'retrieve\\':\\\\\"\\\\n\\'\\\\\\\\n---\\\\\\\\n\\'\\\\n---CHECK DOCUMENT RELEVANCE TO QUESTION---\\\\n---GRADE: DOCUMENT NOT RELEVANT---\\\\n---GRADE: DOCUMENT RELEVANT---\\\\n---GRADE: DOCUMENT NOT RELEVANT---\\\\n---GRADE: DOCUMENT RELEVANT---\\\\n---ASSESS GRADED DOCUMENTS---\\\\n---DECISION: GENERATE---\\\\n\\\\\"Node \\'grade_documents\\':\\\\\"\\\\n\\'\\\\\\\\n---\\\\\\\\n\\'\\\\n---GENERATE---\\\\n---CHECK HALLUCINATIONS---\\\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\\\n---GRADE GENERATION vs QUESTION---\\\\n---DECISION: GENERATION ADDRESSES QUESTION---\\\\n\\\\\"Node \\'generate\\':\\\\\"\\\\n\\'\\\\\\\\n---\\\\\\\\n\\'\\\\n(\\'The types of agent memory include short-term memory, long-term memory, and \\'\\\\n\\'sensory memory. Short-term memory is utilized for in-context learning, while \\'\\\\n\\'long-term memory allows for the retention and recall of information over \\'\\\\n\\'extended periods. Sensory memory involves learning embedding representations \\'\\\\n\\'for various raw inputs, such as text and images.\\')\\\\n\", \"[page 4] Construct the Graph\\\\nCapture the flow in as a graph.\\\\nDefine Graph State\\\\nIn [11]: from typing import List\\\\nfrom typing_extensions import TypedDict\\\\nclass GraphState(TypedDict):\\\\n\\\\\"\\\\\"\\\\\"\\\\nRepresents the state of our graph.\\\\nAttributes:\\\\nquestion: question\\\\ngeneration: LLM generation\\\\ndocuments: list of documents\\\\n\\\\\"\\\\\"\\\\\"\\\\nquestion: str\\\\ngeneration: str\\\\ndocuments: List[str]\\\\nDefine Graph Flow\\\\nIn [12]: from langchain.schema import Document\\\\ndef retrieve(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nRetrieve documents\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstate (dict): New key added to state, documents, that contains retrieved documents\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---RETRIEVE---\\\\\")\\\\nquestion = state[\\\\\"question\\\\\"]\\\\n# Retrieval\\\\ndocuments = retriever.invoke(question)\\\\nreturn {\\\\\"documents\\\\\": documents, \\\\\"question\\\\\": question}\\\\ndef generate(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nGenerate answer\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstate (dict): New key added to state, generation, that contains LLM generation\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---GENERATE---\\\\\")\\\\nquestion = state[\\\\\"question\\\\\"]\\\\ndocuments = state[\\\\\"documents\\\\\"]\\\\n# RAG generation\\\\ngeneration = rag_chain.invoke({\\\\\"context\\\\\": documents, \\\\\"question\\\\\": question})\\\\nreturn {\\\\\"documents\\\\\": documents, \\\\\"question\\\\\": question, \\\\\"generation\\\\\": generation}\\\\ndef grade_documents(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nDetermines whether the retrieved documents are relevant to the question.\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstate (dict): Updates documents key with only filtered relevant documents\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\\\\\")\\\\n\", \"[page 2] )\\\\n)\\\\nprint(question_router.invoke({\\\\\"question\\\\\": \\\\\"What are the types of agent memory?\\\\\"}))\\\\ndatasource=\\'web_search\\'\\\\ndatasource=\\'vectorstore\\'\\\\nIn [5]: ### Retrieval Grader\\\\n# Data model\\\\nclass GradeDocuments(BaseModel):\\\\n\\\\\"\\\\\"\\\\\"Binary score for relevance check on retrieved documents.\\\\\"\\\\\"\\\\\"\\\\nbinary_score: str = Field(\\\\ndescription=\\\\\"Documents are relevant to the question, \\'yes\\' or \\'no\\'\\\\\"\\\\n)\\\\n# LLM with function call\\\\nllm = ChatOpenAI(model=\\\\\"gpt-4o-mini\\\\\", temperature=0)\\\\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\\\\n# Prompt\\\\nsystem = \\\\\"\\\\\"\\\\\"You are a grader assessing relevance of a retrieved document to a user question. \\\\\\\\n\\\\nIf the document contains keyword(s) or semantic meaning related to the user question, grade it as\\\\nIt does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\\\\\n\\\\nGive a binary score \\'yes\\' or \\'no\\' score to indicate whether the document is relevant to the questi\\\\ngrade_prompt = ChatPromptTemplate.from_messages(\\\\n[\\\\n(\\\\\"system\\\\\", system),\\\\n(\\\\\"human\\\\\", \\\\\"Retrieved document: \\\\\\\\n\\\\\\\\n {document} \\\\\\\\n\\\\\\\\n User question: {question}\\\\\"),\\\\n]\\\\n)\\\\nretrieval_grader = grade_prompt | structured_llm_grader\\\\nquestion = \\\\\"agent memory\\\\\"\\\\ndocs = retriever.invoke(question)\\\\ndoc_txt = docs[1].page_content\\\\nprint(retrieval_grader.invoke({\\\\\"question\\\\\": question, \\\\\"document\\\\\": doc_txt}))\\\\nbinary_score=\\'yes\\'\\\\nIn [6]: ### Generate\\\\nfrom langchain import hub\\\\nfrom langchain_core.output_parsers import StrOutputParser\\\\n# Prompt\\\\nprompt = hub.pull(\\\\\"rlm/rag-prompt\\\\\")\\\\n# LLM\\\\nllm = ChatOpenAI(model_name=\\\\\"gpt-4o-mini\\\\\", temperature=0)\\\\n# Post-processing\\\\ndef format_docs(docs):\\\\nreturn \\\\\"\\\\\\\\n\\\\\\\\n\\\\\".join(doc.page_content for doc in docs)\\\\n# Chain\\\\nrag_chain = prompt | llm | StrOutputParser()\\\\n# Run\\\\ngeneration = rag_chain.invoke({\\\\\"context\\\\\": docs, \\\\\"question\\\\\": question})\\\\nprint(generation)\\\\nAgent memory in LLM-powered autonomous systems consists of short-term and long-term memory. Short-term\\\\nmemory utilizes in-context learning for immediate tasks, while long-term memory allows agents to retain\\\\nand recall information over extended periods, often using external storage for efficient retrieval. Thi\\\\ns memory structure supports the agent\\'s ability to reflect on past actions and improve future performan\\\\nce.\\\\nIn [7]: ### Hallucination Grader\\\\n# Data model\\\\nclass GradeHallucinations(BaseModel):\\\\n\\\\\"\\\\\"\\\\\"Binary score for hallucination present in generation answer.\\\\\"\\\\\"\\\\\"\\\\nbinary_score: str = Field(\\\\ndescription=\\\\\"Answer is grounded in the facts, \\'yes\\' or \\'no\\'\\\\\"\\\\n)\\\\n\", \"[page 5] question = state[\\\\\"question\\\\\"]\\\\ndocuments = state[\\\\\"documents\\\\\"]\\\\n# Score each doc\\\\nfiltered_docs = []\\\\nfor d in documents:\\\\nscore = retrieval_grader.invoke(\\\\n{\\\\\"question\\\\\": question, \\\\\"document\\\\\": d.page_content}\\\\n)\\\\ngrade = score.binary_score\\\\nif grade == \\\\\"yes\\\\\":\\\\nprint(\\\\\"---GRADE: DOCUMENT RELEVANT---\\\\\")\\\\nfiltered_docs.append(d)\\\\nelse:\\\\nprint(\\\\\"---GRADE: DOCUMENT NOT RELEVANT---\\\\\")\\\\ncontinue\\\\nreturn {\\\\\"documents\\\\\": filtered_docs, \\\\\"question\\\\\": question}\\\\ndef transform_query(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nTransform the query to produce a better question.\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstate (dict): Updates question key with a re-phrased question\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---TRANSFORM QUERY---\\\\\")\\\\nquestion = state[\\\\\"question\\\\\"]\\\\ndocuments = state[\\\\\"documents\\\\\"]\\\\n# Re-write question\\\\nbetter_question = question_rewriter.invoke({\\\\\"question\\\\\": question})\\\\nreturn {\\\\\"documents\\\\\": documents, \\\\\"question\\\\\": better_question}\\\\ndef web_search(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nWeb search based on the re-phrased question.\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstate (dict): Updates documents key with appended web results\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---WEB SEARCH---\\\\\")\\\\nquestion = state[\\\\\"question\\\\\"]\\\\n# Web search\\\\ndocs = web_search_tool.invoke({\\\\\"query\\\\\": question})\\\\nweb_results = \\\\\"\\\\\\\\n\\\\\".join([d[\\\\\"content\\\\\"] for d in docs])\\\\nweb_results = Document(page_content=web_results)\\\\nreturn {\\\\\"documents\\\\\": web_results, \\\\\"question\\\\\": question}\\\\n### Edges ###\\\\ndef route_question(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nRoute question to web search or RAG.\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstr: Next node to call\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---ROUTE QUESTION---\\\\\")\\\\nquestion = state[\\\\\"question\\\\\"]\\\\nsource = question_router.invoke({\\\\\"question\\\\\": question})\\\\nif source.datasource == \\\\\"web_search\\\\\":\\\\nprint(\\\\\"---ROUTE QUESTION TO WEB SEARCH---\\\\\")\\\\nreturn \\\\\"web_search\\\\\"\\\\nelif source.datasource == \\\\\"vectorstore\\\\\":\\\\nprint(\\\\\"---ROUTE QUESTION TO RAG---\\\\\")\\\\nreturn \\\\\"vectorstore\\\\\"\\\\n\", \"[page 1] from langchain.text_splitter import RecursiveCharacterTextSplitter\\\\nfrom langchain_community.document_loaders import WebBaseLoader\\\\nfrom langchain_community.vectorstores import Chroma\\\\nfrom langchain_openai import OpenAIEmbeddings\\\\n### from langchain_cohere import CohereEmbeddings\\\\n# Set embeddings\\\\nembd = OpenAIEmbeddings()\\\\n# Docs to index\\\\nurls = [\\\\n\\\\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\\\",\\\\n\\\\\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\\\\\",\\\\n\\\\\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\\\\\",\\\\n]\\\\n# Load\\\\ndocs = [WebBaseLoader(url).load() for url in urls]\\\\ndocs_list = [item for sublist in docs for item in sublist]\\\\n# Split\\\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\\\nchunk_size=500, chunk_overlap=0\\\\n)\\\\ndoc_splits = text_splitter.split_documents(docs_list)\\\\n# Add to vectorstore\\\\nvectorstore = Chroma.from_documents(\\\\ndocuments=doc_splits,\\\\ncollection_name=\\\\\"rag-chroma\\\\\",\\\\nembedding=embd,\\\\n)\\\\nretriever = vectorstore.as_retriever()\\\\nLLMs\\\\nUsing Pydantic with LangChain\\\\nThis notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core <\\\\n0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\\\\nIn [4]: ### Router\\\\nfrom typing import Literal\\\\nfrom langchain_core.prompts import ChatPromptTemplate\\\\nfrom langchain_openai import ChatOpenAI\\\\nfrom pydantic import BaseModel, Field\\\\n# Data model\\\\nclass RouteQuery(BaseModel):\\\\n\\\\\"\\\\\"\\\\\"Route a user query to the most relevant datasource.\\\\\"\\\\\"\\\\\"\\\\ndatasource: Literal[\\\\\"vectorstore\\\\\", \\\\\"web_search\\\\\"] = Field(\\\\n...,\\\\ndescription=\\\\\"Given a user question choose to route it to web search or a vectorstore.\\\\\",\\\\n)\\\\n# LLM with function call\\\\nllm = ChatOpenAI(model=\\\\\"gpt-4o-mini\\\\\", temperature=0)\\\\nstructured_llm_router = llm.with_structured_output(RouteQuery)\\\\n# Prompt\\\\nsystem = \\\\\"\\\\\"\\\\\"You are an expert at routing a user question to a vectorstore or web search.\\\\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\\\\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\\\\\"\\\\\"\\\\\"\\\\nroute_prompt = ChatPromptTemplate.from_messages(\\\\n[\\\\n(\\\\\"system\\\\\", system),\\\\n(\\\\\"human\\\\\", \\\\\"{question}\\\\\"),\\\\n]\\\\n)\\\\nquestion_router = route_prompt | structured_llm_router\\\\nprint(\\\\nquestion_router.invoke(\\\\n{\\\\\"question\\\\\": \\\\\"Who will the Bears draft first in the NFL draft?\\\\\"}\\\\n\", \"[page 3] # LLM with function call\\\\nllm = ChatOpenAI(model=\\\\\"gpt-4o-mini\\\\\", temperature=0)\\\\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\\\\n# Prompt\\\\nsystem = \\\\\"\\\\\"\\\\\"You are a grader assessing whether an LLM generation is grounded in / supported by a set o\\\\nGive a binary score \\'yes\\' or \\'no\\'. \\'Yes\\' means that the answer is grounded in / supported by the\\\\nhallucination_prompt = ChatPromptTemplate.from_messages(\\\\n[\\\\n(\\\\\"system\\\\\", system),\\\\n(\\\\\"human\\\\\", \\\\\"Set of facts: \\\\\\\\n\\\\\\\\n {documents} \\\\\\\\n\\\\\\\\n LLM generation: {generation}\\\\\"),\\\\n]\\\\n)\\\\nhallucination_grader = hallucination_prompt | structured_llm_grader\\\\nhallucination_grader.invoke({\\\\\"documents\\\\\": docs, \\\\\"generation\\\\\": generation})\\\\nOut[7]: GradeHallucinations(binary_score=\\'yes\\')\\\\nIn [8]: ### Answer Grader\\\\n# Data model\\\\nclass GradeAnswer(BaseModel):\\\\n\\\\\"\\\\\"\\\\\"Binary score to assess answer addresses question.\\\\\"\\\\\"\\\\\"\\\\nbinary_score: str = Field(\\\\ndescription=\\\\\"Answer addresses the question, \\'yes\\' or \\'no\\'\\\\\"\\\\n)\\\\n# LLM with function call\\\\nllm = ChatOpenAI(model=\\\\\"gpt-4o-mini\\\\\", temperature=0)\\\\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\\\\n# Prompt\\\\nsystem = \\\\\"\\\\\"\\\\\"You are a grader assessing whether an answer addresses / resolves a question \\\\\\\\n\\\\nGive a binary score \\'yes\\' or \\'no\\'. Yes\\' means that the answer resolves the question.\\\\\"\\\\\"\\\\\"\\\\nanswer_prompt = ChatPromptTemplate.from_messages(\\\\n[\\\\n(\\\\\"system\\\\\", system),\\\\n(\\\\\"human\\\\\", \\\\\"User question: \\\\\\\\n\\\\\\\\n {question} \\\\\\\\n\\\\\\\\n LLM generation: {generation}\\\\\"),\\\\n]\\\\n)\\\\nanswer_grader = answer_prompt | structured_llm_grader\\\\nanswer_grader.invoke({\\\\\"question\\\\\": question, \\\\\"generation\\\\\": generation})\\\\nOut[8]: GradeAnswer(binary_score=\\'yes\\')\\\\nIn [9]: ### Question Re-writer\\\\n# LLM\\\\nllm = ChatOpenAI(model=\\\\\"gpt-4o-mini\\\\\", temperature=0)\\\\n# Prompt\\\\nsystem = \\\\\"\\\\\"\\\\\"You a question re-writer that converts an input question to a better version that is optim\\\\nfor vectorstore retrieval. Look at the input and try to reason about the underlying semantic inte\\\\nre_write_prompt = ChatPromptTemplate.from_messages(\\\\n[\\\\n(\\\\\"system\\\\\", system),\\\\n(\\\\n\\\\\"human\\\\\",\\\\n\\\\\"Here is the initial question: \\\\\\\\n\\\\\\\\n {question} \\\\\\\\n Formulate an improved question.\\\\\",\\\\n),\\\\n]\\\\n)\\\\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\\\\nquestion_rewriter.invoke({\\\\\"question\\\\\": question})\\\\nOut[9]: \\'What are the key concepts and techniques related to agent memory in artificial intelligence?\\'\\\\nWeb Search Tool\\\\nIn [10]: ### Search\\\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\\\nweb_search_tool = TavilySearchResults(k=3)\\\\n\", \"[page 7] workflow.add_conditional_edges(\\\\nSTART,\\\\nroute_question,\\\\n{\\\\n\\\\\"web_search\\\\\": \\\\\"web_search\\\\\",\\\\n\\\\\"vectorstore\\\\\": \\\\\"retrieve\\\\\",\\\\n},\\\\n)\\\\nworkflow.add_edge(\\\\\"web_search\\\\\", \\\\\"generate\\\\\")\\\\nworkflow.add_edge(\\\\\"retrieve\\\\\", \\\\\"grade_documents\\\\\")\\\\nworkflow.add_conditional_edges(\\\\n\\\\\"grade_documents\\\\\",\\\\ndecide_to_generate,\\\\n{\\\\n\\\\\"transform_query\\\\\": \\\\\"transform_query\\\\\",\\\\n\\\\\"generate\\\\\": \\\\\"generate\\\\\",\\\\n},\\\\n)\\\\nworkflow.add_edge(\\\\\"transform_query\\\\\", \\\\\"retrieve\\\\\")\\\\nworkflow.add_conditional_edges(\\\\n\\\\\"generate\\\\\",\\\\ngrade_generation_v_documents_and_question,\\\\n{\\\\n\\\\\"not supported\\\\\": \\\\\"generate\\\\\",\\\\n\\\\\"useful\\\\\": END,\\\\n\\\\\"not useful\\\\\": \\\\\"transform_query\\\\\",\\\\n},\\\\n)\\\\n# Compile\\\\napp = workflow.compile()\\\\nUse Graph\\\\nIn [14]: from pprint import pprint\\\\n# Run\\\\ninputs = {\\\\n\\\\\"question\\\\\": \\\\\"What player at the Bears expected to draft first in the 2024 NFL draft?\\\\\"\\\\n}\\\\nfor output in app.stream(inputs):\\\\nfor key, value in output.items():\\\\n# Node\\\\npprint(f\\\\\"Node \\'{key}\\':\\\\\")\\\\n# Optional: print full state at each node\\\\n# pprint.pprint(value[\\\\\"keys\\\\\"], indent=2, width=80, depth=None)\\\\npprint(\\\\\"\\\\\\\\n---\\\\\\\\n\\\\\")\\\\n# Final generation\\\\npprint(value[\\\\\"generation\\\\\"])\\\\n---ROUTE QUESTION---\\\\n---ROUTE QUESTION TO WEB SEARCH---\\\\n---WEB SEARCH---\\\\n\\\\\"Node \\'web_search\\':\\\\\"\\\\n\\'\\\\\\\\n---\\\\\\\\n\\'\\\\n---GENERATE---\\\\n---CHECK HALLUCINATIONS---\\\\n---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\\\n---GRADE GENERATION vs QUESTION---\\\\n---DECISION: GENERATION ADDRESSES QUESTION---\\\\n\\\\\"Node \\'generate\\':\\\\\"\\\\n\\'\\\\\\\\n---\\\\\\\\n\\'\\\\n(\\'The Chicago Bears are expected to draft quarterback Caleb Williams first \\'\\\\n\\'overall in the 2024 NFL Draft. They also have a second first-round pick, \\'\\\\n\\'where they selected wide receiver Rome Odunze.\\')\\\\nIn [15]: # Run\\\\ninputs = {\\\\\"question\\\\\": \\\\\"What are the types of agent memory?\\\\\"}\\\\nfor output in app.stream(inputs):\\\\nfor key, value in output.items():\\\\n# Node\\\\npprint(f\\\\\"Node \\'{key}\\':\\\\\")\\\\n# Optional: print full state at each node\\\\n# pprint.pprint(value[\\\\\"keys\\\\\"], indent=2, width=80, depth=None)\\\\npprint(\\\\\"\\\\\\\\n---\\\\\\\\n\\\\\")\\\\n# Final generation\\\\npprint(value[\\\\\"generation\\\\\"])\\\\n\", \"[page 6] def decide_to_generate(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nDetermines whether to generate an answer, or re-generate a question.\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstr: Binary decision for next node to call\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---ASSESS GRADED DOCUMENTS---\\\\\")\\\\nstate[\\\\\"question\\\\\"]\\\\nfiltered_documents = state[\\\\\"documents\\\\\"]\\\\nif not filtered_documents:\\\\n# All documents have been filtered check_relevance\\\\n# We will re-generate a new query\\\\nprint(\\\\n\\\\\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\\\\\"\\\\n)\\\\nreturn \\\\\"transform_query\\\\\"\\\\nelse:\\\\n# We have relevant documents, so generate answer\\\\nprint(\\\\\"---DECISION: GENERATE---\\\\\")\\\\nreturn \\\\\"generate\\\\\"\\\\ndef grade_generation_v_documents_and_question(state):\\\\n\\\\\"\\\\\"\\\\\"\\\\nDetermines whether the generation is grounded in the document and answers question.\\\\nArgs:\\\\nstate (dict): The current graph state\\\\nReturns:\\\\nstr: Decision for next node to call\\\\n\\\\\"\\\\\"\\\\\"\\\\nprint(\\\\\"---CHECK HALLUCINATIONS---\\\\\")\\\\nquestion = state[\\\\\"question\\\\\"]\\\\ndocuments = state[\\\\\"documents\\\\\"]\\\\ngeneration = state[\\\\\"generation\\\\\"]\\\\nscore = hallucination_grader.invoke(\\\\n{\\\\\"documents\\\\\": documents, \\\\\"generation\\\\\": generation}\\\\n)\\\\ngrade = score.binary_score\\\\n# Check hallucination\\\\nif grade == \\\\\"yes\\\\\":\\\\nprint(\\\\\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\\\\\")\\\\n# Check question-answering\\\\nprint(\\\\\"---GRADE GENERATION vs QUESTION---\\\\\")\\\\nscore = answer_grader.invoke({\\\\\"question\\\\\": question, \\\\\"generation\\\\\": generation})\\\\ngrade = score.binary_score\\\\nif grade == \\\\\"yes\\\\\":\\\\nprint(\\\\\"---DECISION: GENERATION ADDRESSES QUESTION---\\\\\")\\\\nreturn \\\\\"useful\\\\\"\\\\nelse:\\\\nprint(\\\\\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\\\\\")\\\\nreturn \\\\\"not useful\\\\\"\\\\nelse:\\\\npprint(\\\\\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\\\\\")\\\\nreturn \\\\\"not supported\\\\\"\\\\nCompile Graph\\\\nIn [13]: from langgraph.graph import END, StateGraph, START\\\\nworkflow = StateGraph(GraphState)\\\\n# Define the nodes\\\\nworkflow.add_node(\\\\\"web_search\\\\\", web_search) # web search\\\\nworkflow.add_node(\\\\\"retrieve\\\\\", retrieve) # retrieve\\\\nworkflow.add_node(\\\\\"grade_documents\\\\\", grade_documents) # grade documents\\\\nworkflow.add_node(\\\\\"generate\\\\\", generate) # generatae\\\\nworkflow.add_node(\\\\\"transform_query\\\\\", transform_query) # transform_query\\\\n# Build graph\\\\n\"]', name='retrieve', id='30e4af29-178b-48a3-9d23-0beb35f09aa6', tool_call_id='call_oqqHIAbJJBn1j7KcXBrMZvmw'), AIMessage(content='LangGraph에서 Adaptive RAG(정확하고 효율적인 응답 생성을 위한 정보를 검색하는 기법)를 구축하는 방법에 대한 개요는 다음과 같습니다.\\n\\n### 1. 필요한 패키지 설치 및 환경 설정\\n먼저, 필요한 패키지를 설치하고 API 키를 설정합니다.\\n```python\\n%pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb\\nimport getpass\\nimport os\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n_set_env(\"OPENAI_API_KEY\")\\n_set_env(\"TAVILY_API_KEY\")\\n```\\n\\n### 2. 인덱스 생성\\n웹에서 수집한 데이터를 인덱싱합니다. 데이터는 웹 페이지에서 가져올 수 있습니다.\\n```python\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain_community.vectorstores import Chroma\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembd = OpenAIEmbeddings()\\nurls = [\"<웹페이지 URL1>\", \"<웹페이지 URL2>\"]\\ndocs = [WebBaseLoader(url).load() for url in urls]\\ndocs_list = [item for sublist in docs for item in sublist]\\n\\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\\ndoc_splits = text_splitter.split_documents(docs_list)\\n\\nvectorstore = Chroma.from_documents(documents=doc_splits, collection_name=\"rag-chroma\", embedding=embd)\\nretriever = vectorstore.as_retriever()\\n```\\n\\n### 3. 질문 라우팅 및 처리\\n사용자의 질문을 웹 검색 또는 RAG로 라우팅합니다.\\n```python\\ndef route_question(state):\\n    question = state[\"question\"]\\n    source = question_router.invoke({\"question\": question})\\n    if source.datasource == \"web_search\":\\n        return \"web_search\"\\n    elif source.datasource == \"vectorstore\":\\n        return \"retrieve\"\\n```\\n\\n### 4. 문서 검색 및 평가\\n검색된 문서들이 질문과 얼마나 관련 있는지 평가합니다.\\n```python\\ndef grade_documents(state):\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    \\n    filtered_docs = []\\n    for doc in documents:\\n        score = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\\n        if score.binary_score == \"yes\":\\n            filtered_docs.append(doc)\\n\\n    return {\"documents\": filtered_docs, \"question\": question}\\n```\\n\\n### 5. 응답 생성\\n조건에 맞는 문서를 바탕으로 응답을 생성합니다.\\n```python\\ndef generate(state):\\n    question = state[\"question\"]\\n    documents = state[\"documents\"]\\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\\n```\\n\\n### 6. 그래프 정의 및 컴파일\\n모든 단계를 그래프 형태로 연결합니다.\\n```python\\nfrom langgraph.graph import END, StateGraph, START\\n\\nworkflow = StateGraph(GraphState)\\nworkflow.add_node(\"web_search\", web_search) \\nworkflow.add_node(\"retrieve\", retrieve)\\nworkflow.add_node(\"grade_documents\", grade_documents)\\nworkflow.add_node(\"generate\", generate)\\n```\\n\\n### 7. 실행\\n최종적으로 질문을 입력받아 프로세스를 실행합니다.\\n```python\\ninputs = {\"question\": \"질문을 입력하세요\"}\\nfor output in app.stream(inputs):\\n    # 결과 처리\\n```\\n\\n이러한 단계들을 통해 LangGraph에서 Adaptive RAG를 구축할 수 있습니다. 각 단계마다 구체적인 세부사항은 문서화된 코드나 예제를 참고하면 됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 803, 'prompt_tokens': 4832, 'total_tokens': 5635, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-82b1e8a9-7874-4fdb-9e66-fdfd3c98da95-0', usage_metadata={'input_tokens': 4832, 'output_tokens': 803, 'total_tokens': 5635, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "import nest_asyncio  # 이 두개는 jupyter lab에서만\n",
    "nest_asyncio.apply() # 설정하는 함수입니다. py에선 사용안합니다.\n",
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "async def main():\n",
    "    async with MultiServerMCPClient(\n",
    "        {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./server/math_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./server/weather_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\t\t\t\n",
    "        \"PDF_FAISS\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./server/faiss_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "\n",
    "        }\n",
    "    ) as client:\n",
    "        agent = create_react_agent(model, client.get_tools())\n",
    "        math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "        weather_response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})\n",
    "        faiss_response = await agent.ainvoke({\"messages\": \"langgraph에서 adaptive rag를 구축하는 방법을 알려주세요.\"})\n",
    "        print(math_response)\n",
    "        print(weather_response)\n",
    "        print(faiss_response)\n",
    "# asyncio 이벤트 루프를 통해 main() 실행\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
