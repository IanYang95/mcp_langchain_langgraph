{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_response(response):\n",
    "    \"\"\"response ê°ì²´ì˜ messagesì—ì„œ ë§ˆì§€ë§‰ AIMessageì˜ contentë¥¼ ì¶”ì¶œ\"\"\"\n",
    "    messages = response.get('messages', [])\n",
    "    for message in reversed(messages):\n",
    "        if hasattr(message, \"content\") and isinstance(message.content, str) and message.content.strip():\n",
    "            return message.content.strip()\n",
    "    return \"[No final response found]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“˜ Math Response:\n",
      "The result of \\((3 + 5) \\times 12\\) is 96.\n",
      "\n",
      "ğŸŒ¤ï¸ Weather Response:\n",
      "The weather in New York City is always sunny!\n",
      "\n",
      "ğŸ§  FAISS / LangGraph Response:\n",
      "Adaptive RAG (Retrieval-Augmented Generation) in LangGraph can be constructed by combining query analysis and an active/self-corrective approach to enhance the retrieval results based on user queries. Here's a step-by-step guide to implementing Adaptive RAG in LangGraph:\n",
      "\n",
      "### 1. Set Up Your Environment\n",
      "First, you need to install the necessary packages and set up your API keys:\n",
      "\n",
      "```python\n",
      "!pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb\n",
      "\n",
      "import getpass\n",
      "import os\n",
      "\n",
      "def _set_env(var: str):\n",
      "    if not os.environ.get(var):\n",
      "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
      "\n",
      "_set_env(\"OPENAI_API_KEY\")\n",
      "_set_env(\"TAVILY_API_KEY\")\n",
      "```\n",
      "\n",
      "### 2. Create an Index\n",
      "Start by creating an index for your documents. This is essential for the retrieval system.\n",
      "\n",
      "```python\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "from langchain_community.vectorstores import Chroma\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "\n",
      "# Set embeddings\n",
      "embd = OpenAIEmbeddings()\n",
      "\n",
      "# Load documents from URLs\n",
      "urls = [\n",
      "    \"https://example.com/document1\",\n",
      "    \"https://example.com/document2\",\n",
      "    # Add more URLs as needed\n",
      "]\n",
      "\n",
      "# Load and split documents\n",
      "docs = [WebBaseLoader(url).load() for url in urls]\n",
      "doc_splits = RecursiveCharacterTextSplitter().split_documents(docs)\n",
      "\n",
      "# Add to vectorstore\n",
      "vectorstore = Chroma.from_documents(documents=doc_splits, embedding=embd)\n",
      "retriever = vectorstore.as_retriever()\n",
      "```\n",
      "\n",
      "### 3. Define the Graph State\n",
      "Create a state representation to keep track of the current question, generated answers, and relevant documents.\n",
      "\n",
      "```python\n",
      "from typing import List\n",
      "from typing_extensions import TypedDict\n",
      "\n",
      "class GraphState(TypedDict):\n",
      "    question: str\n",
      "    generation: str\n",
      "    documents: List[str]\n",
      "```\n",
      "\n",
      "### 4. Define Graph Flow\n",
      "Implement the key functions involved in the RAG process.\n",
      "\n",
      "1. **Retrieve Documents**: Retrieve relevant documents based on the question.\n",
      "2. **Generate Answer**: Generate an answer based on the retrieved documents.\n",
      "3. **Grade Documents**: Determine if retrieved documents are relevant to the question.\n",
      "\n",
      "Example of the retrieval function:\n",
      "\n",
      "```python\n",
      "def retrieve(state: GraphState):\n",
      "    print(\"---RETRIEVE---\")\n",
      "    question = state[\"question\"]\n",
      "    documents = retriever.invoke(question)  # Call your retriever function here\n",
      "    return {\"documents\": documents, \"question\": question}\n",
      "```\n",
      "\n",
      "### 5. Route Questions\n",
      "Route the questions to either web search or vectorstore based on their content.\n",
      "\n",
      "```python\n",
      "def route_question(state: GraphState):\n",
      "    question = state[\"question\"]\n",
      "    # Logic to determine if we use web search or vectorstore based on the question\n",
      "    if is_recent_event(question):\n",
      "        return \"web_search\"\n",
      "    else:\n",
      "        return \"retrieve\"\n",
      "```\n",
      "\n",
      "### 6. Web Search Integration\n",
      "Implement a function to perform web searches for questions concerning recent events.\n",
      "\n",
      "```python\n",
      "def web_search(state: GraphState):\n",
      "    print(\"---WEB SEARCH---\")\n",
      "    question = state[\"question\"]\n",
      "    # Logic to perform web search\n",
      "    return {\"documents\": web_search_results, \"question\": question}\n",
      "```\n",
      "\n",
      "### 7. Create Graph Workflow\n",
      "Combine all components into a workflow that processes questions through various states.\n",
      "\n",
      "```python\n",
      "from langchain_core.workflow import Workflow\n",
      "\n",
      "workflow = Workflow()\n",
      "workflow.add_edge(\"start\", \"route_question\")\n",
      "workflow.add_edge(\"route_question\", \"web_search\")\n",
      "workflow.add_edge(\"route_question\", \"retrieve\")\n",
      "# Add more edges as needed to complete workflow\n",
      "\n",
      "app = workflow.compile()\n",
      "```\n",
      "\n",
      "### 8. Run and Test\n",
      "Execute the workflow with a sample question to see how it functions.\n",
      "\n",
      "```python\n",
      "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
      "for output in app.stream(inputs):\n",
      "    print(output)\n",
      "```\n",
      "\n",
      "This framework should help you implement Adaptive RAG in LangGraph effectively. Adjust and optimize based on your specific use case and requirements.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "import nest_asyncio  # ì´ ë‘ê°œëŠ” jupyter labì—ì„œë§Œ\n",
    "nest_asyncio.apply() # ì„¤ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. pyì—ì„  ì‚¬ìš©ì•ˆí•©ë‹ˆë‹¤.\n",
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY ì •ë³´ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "async def main():\n",
    "    async with MultiServerMCPClient(\n",
    "        {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./server/math_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./server/weather_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\t\t\t\n",
    "        \"PDF_FAISS\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./server/faiss_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "\n",
    "        }\n",
    "    ) as client:\n",
    "        agent = create_react_agent(model, client.get_tools())\n",
    "        math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "        weather_response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})\n",
    "        faiss_response = await agent.ainvoke({\"messages\": \"langgraphì—ì„œ adaptive ragë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.\"})\n",
    "        # ê° ì‘ë‹µì—ì„œ í•µì‹¬ ì‘ë‹µ ì¶œë ¥\n",
    "        print(\"ğŸ“˜ Math Response:\")\n",
    "        print(extract_final_response(math_response))\n",
    "        print(\"\\nğŸŒ¤ï¸ Weather Response:\")\n",
    "        print(extract_final_response(weather_response))\n",
    "        print(\"\\nğŸ§  FAISS / LangGraph Response:\")\n",
    "        print(extract_final_response(faiss_response))\n",
    "# asyncio ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ í†µí•´ main() ì‹¤í–‰\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
